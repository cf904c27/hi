{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_1.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_date = df.columns[1]\n",
    "data_end_date = df.columns[-1]\n",
    "print('Data ranges from %s to %s' % (data_start_date, data_end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_series(df, n_series):\n",
    "    \n",
    "    sample = df.sample(n_series, random_state=8)\n",
    "    page_labels = sample['Page'].tolist()\n",
    "    series_samples = sample.loc[:,data_start_date:data_end_date]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    for i in range(series_samples.shape[0]):\n",
    "        np.log1p(pd.Series(series_samples.iloc[i]).astype(np.float64)).plot(linewidth=1.5)\n",
    "    \n",
    "    plt.title('Randomly Selected Wikipedia Page Daily Views Over Time (Log(views) + 1)')\n",
    "    plt.legend(page_labels)\n",
    "    \n",
    "plot_random_series(df, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整理資料格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "pred_steps = 14\n",
    "pred_length=timedelta(pred_steps)\n",
    "\n",
    "first_day = pd.to_datetime(data_start_date) \n",
    "last_day = pd.to_datetime(data_end_date)\n",
    "\n",
    "val_pred_start = last_day - pred_length + timedelta(1)\n",
    "val_pred_end = last_day\n",
    "\n",
    "train_pred_start = val_pred_start - pred_length\n",
    "train_pred_end = val_pred_start - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_length = train_pred_start - first_day\n",
    "\n",
    "train_enc_start = first_day\n",
    "train_enc_end = train_enc_start + enc_length - timedelta(1)\n",
    "\n",
    "val_enc_start = train_enc_start + pred_length\n",
    "val_enc_end = val_enc_start + enc_length - timedelta(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割訓練、驗證樣本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train encoding:', train_enc_start, '-', train_enc_end)\n",
    "print('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\n",
    "print('Val encoding:', val_enc_start, '-', val_enc_end)\n",
    "print('Val prediction:', val_pred_start, '-', val_pred_end)\n",
    "\n",
    "print('\\nEncoding interval:', enc_length.days)\n",
    "print('Prediction interval:', pred_length.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in df.columns[1:]]),\n",
    "                          data=[i for i in range(len(df.columns[1:]))])\n",
    "\n",
    "series_array = df[df.columns[1:]].values\n",
    "\n",
    "def get_time_block_series(series_array, date_to_index, start_date, end_date):\n",
    "    \n",
    "    inds = date_to_index[start_date:end_date]\n",
    "    return series_array[:,inds]\n",
    "\n",
    "def transform_series_encode(series_array):\n",
    "    \n",
    "    series_array = np.log1p(series_array)\n",
    "    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n",
    "    \n",
    "    return series_array\n",
    "\n",
    "def transform_series_decode(series_array):\n",
    "    \n",
    "    series_array = np.log1p(series_array)\n",
    "    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n",
    "    \n",
    "    return series_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n_samples = 50000\n",
    "\n",
    "# 取前50000個樣本，並且限制其包含的時間範圍  \n",
    "encoder_input_data = get_time_block_series(series_array, date_to_index, \n",
    "                                           train_enc_start, train_enc_end)[:first_n_samples]\n",
    "encoder_input_data = transform_series_encode(encoder_input_data)\n",
    "\n",
    "# 取前50000個樣本，並且限制其包含的時間範圍 \n",
    "decoder_target_data = get_time_block_series(series_array, date_to_index, \n",
    "                                            train_pred_start, train_pred_end)[:first_n_samples]\n",
    "decoder_target_data = transform_series_decode(decoder_target_data)\n",
    "\n",
    "# lagged target series for teacher forcing\n",
    "decoder_input_data = np.zeros(decoder_target_data.shape)\n",
    "decoder_input_data[:,1:,0] = decoder_target_data[:,:-1,0]\n",
    "decoder_input_data[:,0,0] = encoder_input_data[:,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "latent_dim = 50 # LSTM hidden units\n",
    "#Encoder \n",
    "encoder_inputs = Input(shape=(None, 1)) \n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# 只保留最後時刻的hidden state 和 cell state\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(None, 1))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)  #decoder的cell state用 encoder_states初始化\n",
    "decoder_dense = Dense(1)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#定義訓練時的模型\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義預測時的模型\n",
    "# Encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "# Decoder\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mae')\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend(['Train','Valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):    \n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 1))\n",
    "    \n",
    "    # Populate the first target sequence with end of encoding series pageviews\n",
    "    target_seq[0, 0, 0] = input_seq[0, -1, 0]\n",
    "\n",
    "    # Sampling loop for a batch of sequences - we will fill decoded_seq with predictions\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "\n",
    "    decoded_seq = np.zeros((1,pred_steps,1))\n",
    "    \n",
    "    for i in range(pred_steps):\n",
    "        \n",
    "        output, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        decoded_seq[0,i,0] = output[0,0,0]\n",
    "                # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 1))\n",
    "        target_seq[0, 0, 0] = output[0,0,0]\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#驗證樣本轉換資料格式\n",
    "encoder_input_data = get_time_block_series(series_array, date_to_index, val_enc_start, val_enc_end)\n",
    "encoder_input_data = transform_series_encode(encoder_input_data)\n",
    "\n",
    "decoder_target_data = get_time_block_series(series_array, date_to_index, val_pred_start, val_pred_end)\n",
    "decoder_target_data = transform_series_decode(decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(encoder_input_data, decoder_target_data, sample_ind, enc_tail_len=50):\n",
    "\n",
    "    encode_series = encoder_input_data[sample_ind:sample_ind+1,:,:] \n",
    "    pred_series = decode_sequence(encode_series)\n",
    "    \n",
    "    encode_series = encode_series.reshape(-1,1)\n",
    "    pred_series = pred_series.reshape(-1,1)   \n",
    "    target_series = decoder_target_data[sample_ind,:,:1].reshape(-1,1) \n",
    "    \n",
    "    encode_series_tail = np.concatenate([encode_series[-enc_tail_len:],target_series[:1]])\n",
    "    x_encode = encode_series_tail.shape[0]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))   \n",
    "    \n",
    "    plt.plot(range(1,x_encode+1),encode_series_tail)\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n",
    "    \n",
    "    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n",
    "    plt.legend(['Encoding Series','Target Series','Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_plot(encoder_input_data, decoder_target_data, 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取大資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('spindetail.tsv', sep='\\t',names=col, \n",
    "                 usecols=['PLAYERID','LOCAL_TIME'],parse_dates=['LOCAL_TIME']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data = pd.read_table('NM_SLOT_Statistic_spinDetail.txt', names=['column_name'])\n",
    "new = col_data[\"column_name\"].str.split(\" \", expand = True) \n",
    "col_data[\"new Name\"]= new[2]  \n",
    "col_data = col_data[['new Name']]\n",
    "col = col_data['new Name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19500000it [02:14, 195002.70it/s]                                                                                      \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "processed = pd.DataFrame()\n",
    "chunksize = 10**5\n",
    "with tqdm(total=19447727) as pbar:\n",
    "    for chunk in pd.read_csv('spindetail.tsv', sep='\\t', chunksize=chunksize, names=col, usecols=['PLAYERID','LOCAL_TIME'],parse_dates=['LOCAL_TIME']):\n",
    "        chunk['LOCAL_TIME'] = chunk['LOCAL_TIME'].dt.floor('D') #修正時間格式為年月日\n",
    "        chunk = chunk.drop_duplicates('PLAYERID',keep='first')  #去除重複的玩家id\n",
    "        processed = processed.append(chunk)                     #將結果保存在變數內\n",
    "        pbar.update(chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 150787 entries, 0 to 19447725\n",
      "Data columns (total 2 columns):\n",
      "PLAYERID      150787 non-null object\n",
      "LOCAL_TIME    150787 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dask = dd.read_csv('spindetail.tsv', sep='\\t',names=col,parse_dates=['LOCAL_TIME'], assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GAMESEQNO</th>\n",
       "      <th>SESSIONNO</th>\n",
       "      <th>PLAYERID</th>\n",
       "      <th>PARTNERID</th>\n",
       "      <th>WEBID</th>\n",
       "      <th>MNUM</th>\n",
       "      <th>MTYPE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ACCDENOM</th>\n",
       "      <th>PLAYDENOM</th>\n",
       "      <th>...</th>\n",
       "      <th>SERVER_ID</th>\n",
       "      <th>SETTLE_TYPE</th>\n",
       "      <th>HAS_BONUS</th>\n",
       "      <th>TRANSFER_ID</th>\n",
       "      <th>REPORT_DATE</th>\n",
       "      <th>GAME_TYPE</th>\n",
       "      <th>WIN_TYPE_COMBINATION</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>HOUSEID</th>\n",
       "      <th>LOCAL_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.288066e+12</td>\n",
       "      <td>4d1bb7d4fa8f6221be317ef9396fb7b2ccff6c35</td>\n",
       "      <td>jack8263170@GP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8049.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-11-01 00:00:00.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GP</td>\n",
       "      <td>gphse</td>\n",
       "      <td>2018-11-02 04:31:07.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.288066e+12</td>\n",
       "      <td>eec6d07a34eae701479a9af7c4891966818a4cea</td>\n",
       "      <td>a9am2261017@GP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8049.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-11-01 00:00:00.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GP</td>\n",
       "      <td>gphse</td>\n",
       "      <td>2018-11-02 04:31:08.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.288066e+12</td>\n",
       "      <td>5329e5b7b80661eea3b0b18ad899ac7bf14086c0</td>\n",
       "      <td>thbtbank2425@GH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8049.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-11-01 00:00:00.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GH</td>\n",
       "      <td>ghhse</td>\n",
       "      <td>2018-11-02 04:31:08.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.288066e+12</td>\n",
       "      <td>e1f011dd97a36afbb8c85a1a7be0f704caa4bb61</td>\n",
       "      <td>xpj88216279@GP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8049.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-11-01 00:00:00.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GP</td>\n",
       "      <td>gphse</td>\n",
       "      <td>2018-11-02 04:31:08.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.288066e+12</td>\n",
       "      <td>fc36fd61d40b56befc497ec4094a7de681c3c415</td>\n",
       "      <td>xin781594712@GP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8049.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-11-01 00:00:00.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GP</td>\n",
       "      <td>gphse</td>\n",
       "      <td>2018-11-02 04:31:08.899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GAMESEQNO                                 SESSIONNO         PLAYERID  \\\n",
       "0  7.288066e+12  4d1bb7d4fa8f6221be317ef9396fb7b2ccff6c35   jack8263170@GP   \n",
       "1  7.288066e+12  eec6d07a34eae701479a9af7c4891966818a4cea   a9am2261017@GP   \n",
       "2  7.288066e+12  5329e5b7b80661eea3b0b18ad899ac7bf14086c0  thbtbank2425@GH   \n",
       "3  7.288066e+12  e1f011dd97a36afbb8c85a1a7be0f704caa4bb61   xpj88216279@GP   \n",
       "4  7.288066e+12  fc36fd61d40b56befc497ec4094a7de681c3c415  xin781594712@GP   \n",
       "\n",
       "   PARTNERID  WEBID  MNUM   MTYPE  LOCATION  ACCDENOM  PLAYDENOM  \\\n",
       "0        NaN    NaN   NaN  8049.0       NaN       1.0        1.0   \n",
       "1        NaN    NaN   NaN  8049.0       NaN       1.0        5.0   \n",
       "2        NaN    NaN   NaN  8049.0       NaN       1.0        1.0   \n",
       "3        NaN    NaN   NaN  8049.0       NaN       1.0        1.0   \n",
       "4        NaN    NaN   NaN  8049.0       NaN       1.0        1.0   \n",
       "\n",
       "            ...            SERVER_ID  SETTLE_TYPE  HAS_BONUS TRANSFER_ID  \\\n",
       "0           ...                  NaN          0.0        0.0         0.0   \n",
       "1           ...                  NaN          0.0        0.0         0.0   \n",
       "2           ...                  NaN          0.0        0.0         0.0   \n",
       "3           ...                  NaN          0.0        0.0         0.0   \n",
       "4           ...                  NaN          0.0        0.0         0.0   \n",
       "\n",
       "                  REPORT_DATE  GAME_TYPE  WIN_TYPE_COMBINATION  DOMAIN  \\\n",
       "0  2018-11-01 00:00:00.000000        8.0                   1.0      GP   \n",
       "1  2018-11-01 00:00:00.000000        8.0                   1.0      GP   \n",
       "2  2018-11-01 00:00:00.000000        8.0                   1.0      GH   \n",
       "3  2018-11-01 00:00:00.000000        8.0                   1.0      GP   \n",
       "4  2018-11-01 00:00:00.000000        8.0                   1.0      GP   \n",
       "\n",
       "   HOUSEID              LOCAL_TIME  \n",
       "0    gphse 2018-11-02 04:31:07.806  \n",
       "1    gphse 2018-11-02 04:31:08.082  \n",
       "2    ghhse 2018-11-02 04:31:08.869  \n",
       "3    gphse 2018-11-02 04:31:08.874  \n",
       "4    gphse 2018-11-02 04:31:08.899  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = data_dask.PLAYERID.unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44857"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
